# ML Edge Project with AWS
## ğŸ“Œ Project Goal

Fine-tune a small language model (SLM) for intent classification, then build an AWS-based pipeline that:

Trains & evaluates the model,

Registers versions with a Model Registry,

Compiles the model for the edge with AWS Neo,

Deploys to a device using AWS IoT Greengrass v2.

## ğŸ—‚ï¸ Repo Layout
```bash
slm-edge-mlops/
â”œâ”€ pipelines/
â”‚  â”œâ”€ build_pipeline.py
â”‚  â””â”€ params.json
â”œâ”€ src/
â”‚  â”œâ”€ processing/prepare_data.py
â”‚  â”œâ”€ training/train.py
â”‚  â”œâ”€ evaluation/evaluate.py
â”‚  â””â”€ inference/handler.py
â”œâ”€ edge/
â”‚  â”œâ”€ app/greengrass_infer.py
â”‚  â”œâ”€ recipe.yaml
â”‚  â””â”€ requirements.txt
â”œâ”€ tests/
â”‚  â”œâ”€ test_processing.py
â”‚  â”œâ”€ test_training.py
â”‚  â”œâ”€ test_evaluation.py
â”‚  â””â”€ test_edge_smoke.py
â”œâ”€ gx/        # Great Expectations project
â”œâ”€ infra/     # S3, IAM templates
â””â”€ README.md
```
## ğŸ› ï¸ Step 1 â€” Data Preparation & Validation

Choose a small dataset (e.g., SNIPS intents).

Format as CSV: text,intent.

Add Great Expectations checks:

No missing rows.

Labels in whitelist.

Train/val/test split ratios.

Store in S3:

s3://slm-intents/raw/

s3://slm-intents/processed/

## ğŸ› ï¸ Step 2 â€” Build the Pipeline

Use SageMaker Pipelines:

ProcessingStep â†’ run prepare_data.py

TrainingStep â†’ run train.py (fine-tunes a MiniLM/TinyBERT)

ProcessingStep (Evaluation) â†’ run evaluate.py and save metrics.json

ConditionStep â†’ only continue if F1 â‰¥ threshold

RegisterModel â†’ push model to Model Registry (stage = â€œStagingâ€)

CompilationStep (Neo) â†’ compile for linux_arm64 (edge target)

## ğŸ› ï¸ Step 3 â€” Model Registry & Promotion

Approve â€œStagingâ€ models into â€œProdâ€ once metrics look good.

Registry tracks lineage, metrics, artifacts.

Store Neo-compiled artifact in S3 under /edge/.

## ğŸ› ï¸ Step 4 â€” Edge Deployment

Configure AWS IoT Greengrass v2 core device.

Create two components:

Model component: Neo-compiled model artifact.

App component: greengrass_infer.py for inference.

Deploy via Greengrass console/CLI to the edge device.

## ğŸ› ï¸ Step 5 â€” Testing

Data tests: Great Expectations checks run in ProcessingStep.

Unit tests: pytest for prepare_data.py, tokenization correctness.

Eval tests: metrics â‰¥ threshold, validated by ConditionStep.

Edge smoke tests: run inference on-device; check latency & RAM.

## ğŸ› ï¸ Step 6 â€” CI/CD Integration

Option 1: Pure SageMaker Pipelines for end-to-end orchestration.

Option 2: CodePipeline + CodeBuild â†’ trigger pipeline, run tests, approve promotion.

## ğŸŒŸ Stretch Goals

Add quantization (INT8) for faster inference.

Collect edge telemetry (confidence scores, drift signals).

Run a shadow deployment (compare cloud vs edge outputs).

Staged rollouts across fleets with Greengrass deployment groups.

## âš¡ With this plan, you cover:

ML basics (train/test split, metrics, overfitting),

AWS ML stack (SageMaker Pipelines, Registry, Neo),

Edge deployment (Greengrass),

MLOps practices (tests, promotion gates).